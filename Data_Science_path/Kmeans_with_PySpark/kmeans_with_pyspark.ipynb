{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans with PySpark\n",
    "## author: Konstantinos Nikopoulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.options(header=False,inferSchema=True).csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|   _c0|_c1|\n",
      "+------+---+\n",
      "|4074.0|928|\n",
      "| 635.0|935|\n",
      "|1392.0|562|\n",
      "|4002.0|149|\n",
      "|3394.0|777|\n",
      "+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale to 0-1 :\n",
      "+------+---+----------+----------+\n",
      "|   _c0|_c1|_c0_Scaled|_c1_Scaled|\n",
      "+------+---+----------+----------+\n",
      "|4074.0|928|     0.851|     0.937|\n",
      "| 635.0|935|     0.133|     0.945|\n",
      "|1392.0|562|     0.291|     0.517|\n",
      "|4002.0|149|     0.836|     0.044|\n",
      "|3394.0|777|     0.709|     0.764|\n",
      "+------+---+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vector to double type\n",
    "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
    "\n",
    "for i in [\"_c0\", \"_c1\"]:\n",
    "    # VectorAssembler Transformation - Converting column to vector type\n",
    "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
    "    # MinMaxScaler Transformation\n",
    "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n",
    "    # Pipeline of VectorAssembler and MinMaxScaler\n",
    "    pipeline = Pipeline(stages=[assembler, scaler])\n",
    "    # Fitting pipeline on dataframe\n",
    "    df = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n",
    "\n",
    "print(\"Scale to 0-1 :\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge columns to create column \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge columns :\n",
      "+------+---+----------+----------+-------------+\n",
      "|   _c0|_c1|_c0_Scaled|_c1_Scaled|     features|\n",
      "+------+---+----------+----------+-------------+\n",
      "|4074.0|928|     0.851|     0.937|[0.851,0.937]|\n",
      "| 635.0|935|     0.133|     0.945|[0.133,0.945]|\n",
      "|1392.0|562|     0.291|     0.517|[0.291,0.517]|\n",
      "|4002.0|149|     0.836|     0.044|[0.836,0.044]|\n",
      "|3394.0|777|     0.709|     0.764|[0.709,0.764]|\n",
      "+------+---+----------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"_c0_Scaled\", \"_c1_Scaled\"], outputCol=\"features\")\n",
    "df = vecAssembler.transform(df)\n",
    "\n",
    "print(\"Merge columns :\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose optimal number of clusters using silhouette method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k:  5\n",
      "Silhouette with squared euclidean distance =  0.7796536458687179\n"
     ]
    }
   ],
   "source": [
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = np.zeros(10)\n",
    "for k in range(2,10):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df.sample(False,0.1, seed=42))\n",
    "    predictions = model.transform(df)\n",
    "    silhouette[k] = evaluator.evaluate(predictions) \n",
    "best_k = np.argmax(silhouette)\n",
    "\n",
    "print(\"Best k: \",best_k + 1)\n",
    "print(\"Silhouette with squared euclidean distance = \",silhouette[best_k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Clustering :\n",
      "+------+---+----------+----------+-------------+----------+\n",
      "|   _c0|_c1|_c0_Scaled|_c1_Scaled|     features|prediction|\n",
      "+------+---+----------+----------+-------------+----------+\n",
      "|4074.0|928|     0.851|     0.937|[0.851,0.937]|         1|\n",
      "| 635.0|935|     0.133|     0.945|[0.133,0.945]|         2|\n",
      "|1392.0|562|     0.291|     0.517|[0.291,0.517]|         2|\n",
      "|4002.0|149|     0.836|     0.044|[0.836,0.044]|         3|\n",
      "|3394.0|777|     0.709|     0.764|[0.709,0.764]|         1|\n",
      "+------+---+----------+----------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(k=best_k, seed=1)  \n",
    "model = kmeans.fit(df.select('features'))\n",
    "\n",
    "predictions = model.transform(df)\n",
    "print(\"After Clustering :\")\n",
    "predictions.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
